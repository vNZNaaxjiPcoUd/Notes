# wget

wget \
--recursive \
--no-clobber \
--page-requisites \
--html-extension \
--convert-links \
--restrict-file-names=windows \
--domains [website.org](http://website.org/) \
--no-parent \
-e robots=off \
[www.website.org/tutorials/html/](http://www.website.org/tutorials/html/)

The options are:

- -recursive: download the entire Web site.
- -domains [website.org](http://website.org/): don't follow links outside [website.org](http://website.org/).
- -no-parent: don't follow links outside the directory tutorials/html/.
- -page-requisites: get all the elements that compose the page (images, CSS and so on).
- -html-extension: save files with the .html extension.
- -convert-links: convert links so that they work locally, off-line.
- -restrict-file-names=windows: modify filenames so that they will work in Windows as well.
- -no-clobber: don't overwrite any existing files (used in case the download is interrupted and resumed).
- e robots=off causes it to ignore robots.txt for that domain

ex:
wget \
--recursive \
--no-clobber \
--page-requisites \
--html-extension \
--convert-links \
--restrict-file-names=windows \
--domains [haodoo.net](http://haodoo.net/) \
--no-parent \
-e robots=off \
[www.haodoo.net/](http://www.haodoo.net/)